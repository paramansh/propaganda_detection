{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Propaganda_detection_gpt.ipynb","provenance":[{"file_id":"15oan7cjDITm_FU5OJxDUZW_TFGC8FJ7z","timestamp":1582652832151}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2WkH2d0SLPeT","colab_type":"code","outputId":"87000ea8-a201-43aa-ef04-ab565b7871da","executionInfo":{"status":"ok","timestamp":1582784730594,"user_tz":-330,"elapsed":47865,"user":{"displayName":"Paramansh Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mANS3lCUfabnCwNFz8A_Mb-FC_uqCl9Y4r-zhTcvA=s64","userId":"05094200950482431055"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"49fpc1iF6jcS","colab_type":"code","outputId":"e549b7a3-5632-4c74-9393-5788c68f326c","executionInfo":{"status":"ok","timestamp":1582784751892,"user_tz":-330,"elapsed":5807,"user":{"displayName":"Paramansh Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mANS3lCUfabnCwNFz8A_Mb-FC_uqCl9Y4r-zhTcvA=s64","userId":"05094200950482431055"}},"colab":{"base_uri":"https://localhost:8080/","height":404}},"source":["!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SDlMmtRU1MDJ","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import torch\n","import csv\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6CIN9YGU-qY1","colab_type":"code","colab":{}},"source":["home_dir = \"gdrive/My Drive/propaganda_detection\"\n","data_dir = os.path.join(home_dir, \"datasets\")\n","model_dir = os.path.join(home_dir, \"model_dir\")\n","if not os.path.isdir(model_dir):\n","  os.mkdir(model_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-H93QqgFyxL1","colab_type":"code","colab":{}},"source":["# Read training articles\n","def read_articles(article_dir):\n","  articles = []\n","  train_dir = os.path.join(data_dir, article_dir)\n","  for filename in sorted(os.listdir(train_dir)):\n","    myfile = open(os.path.join(train_dir, filename))\n","    article = myfile.read()\n","    articles.append(article)\n","    myfile.close()\n","  article_ids = []\n","  for filename in sorted(os.listdir(train_dir)):\n","    article_ids.append(filename[7:-4])\n","  return articles, article_ids"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfBpePjiH_-o","colab_type":"code","colab":{}},"source":["# Read training span labels \n","def read_spans():\n","  spans = []\n","  label_dir = os.path.join(data_dir, \"train-labels-task1-span-identification\")\n","  for filename in sorted(os.listdir(label_dir)):\n","    myfile = open(os.path.join(label_dir, filename))\n","    tsvreader = csv.reader(myfile, delimiter=\"\\t\")\n","    span = []\n","    for row in tsvreader:\n","      span.append((int(row[1]), int(row[2])))\n","    myfile.close()\n","    spans.append(span)\n","  return spans"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RLyM6nBECUgl","colab_type":"code","colab":{}},"source":["def print_spans(article, span):\n","  for sp in span:\n","    print (article[sp[0]: sp[1]])\n","  print()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qw8VueZ0iQ9o","colab_type":"code","colab":{}},"source":["class example_sentence:\n","  def __init__(self):\n","    self.tokens = []\n","    self.labels = []\n","    self.article_index = -1 # index of the article to which the sentence is associated\n","    self.index = -1 # index of the sentence in that article \n","    self.word_to_start_char_offset = []\n","    self.word_to_end_char_offset = []\n","  \n","  def __str__(self):\n","    print(\"tokens -\", self.tokens)\n","    print(\"labels -\", self.labels)\n","    print(\"article_index -\", self.article_index)\n","    print(\"index -\", self.index)\n","    print(\"start_offset -\", self.word_to_start_char_offset)\n","    print(\"end_offset -\", self.word_to_end_char_offset)   \n","    return \"\"    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TI57Sv2DrVQ5","colab_type":"code","colab":{}},"source":["def is_whitespace(c):\n","  if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n","    return True\n","  return False\n","\n","def get_sentence_tokens_labels(article, span=None, article_index=None):\n","  doc_tokens = []\n","  char_to_word_offset = []\n","  current_sentence_tokens = [] # actually all sentence tokens for particular article. #TODO rename\n","  word_to_start_char_offset = {}\n","  word_to_end_char_offset = {}\n","  prev_is_whitespace = True\n","  prev_is_newline = True\n","  current_word_position = None\n","  for index, c in enumerate(article):\n","    if c == \"\\n\":\n","      prev_is_newline = True\n","      # check for empty lists\n","      if doc_tokens:\n","        current_sentence_tokens.append(doc_tokens)\n","      doc_tokens = []\n","    if is_whitespace(c):\n","      prev_is_whitespace = True\n","      if current_word_position is not None:\n","        word_to_end_char_offset[current_word_position] = index\n","        current_word_position = None\n","    else:\n","      if prev_is_whitespace:\n","        doc_tokens.append(c)\n","        current_word_position = (len(current_sentence_tokens), len(doc_tokens) - 1)\n","        word_to_start_char_offset[current_word_position] = index # start offset of word\n","      else:\n","        doc_tokens[-1] += c\n","      prev_is_whitespace = False\n","    char_to_word_offset.append((len(current_sentence_tokens), len(doc_tokens) - 1))\n","  if doc_tokens:\n","    current_sentence_tokens.append(doc_tokens)\n","  if current_word_position is not None:\n","    word_to_end_char_offset[current_word_position] = index\n","    current_word_position = None\n","  if span is None:\n","    return current_sentence_tokens, (word_to_start_char_offset, word_to_end_char_offset)\n","\n","  current_propaganda_labels = []\n","  for doc_tokens in current_sentence_tokens:\n","    current_propaganda_labels.append([0] * len(doc_tokens))\n","\n","  start_positions = []\n","  end_positions = []\n","\n","  for sp in span:\n","    if (char_to_word_offset[sp[0]][0] != char_to_word_offset[sp[1]-1][0]):\n","      l1 = char_to_word_offset[sp[0]][0]\n","      l2 = char_to_word_offset[sp[1] - 1][0]\n","      start_positions.append(char_to_word_offset[sp[0]])\n","      end_positions.append((l1, len(current_sentence_tokens[l1])-1))\n","      l1 += 1\n","      while(l1 < l2):\n","        start_positions.append((l1, 0))\n","        end_positions.append((l1, len(current_sentence_tokens[l1])-1))\n","        l1 += 1\n","      start_positions.append((l2, 0))\n","      end_positions.append(char_to_word_offset[sp[1]-1])  \n","      continue\n","    start_positions.append(char_to_word_offset[sp[0]])\n","    end_positions.append(char_to_word_offset[sp[1]-1])\n","\n","  for i, s in enumerate(start_positions):\n","    assert start_positions[i][0] == end_positions[i][0]\n","    if TAGGING_SCHEME == \"BIO\":\n","      current_propaganda_labels[start_positions[i][0]][start_positions[i][1]] = 2 # Begin label\n","      if start_positions[i][1] < end_positions[i][1]:\n","        current_propaganda_labels[start_positions[i][0]][start_positions[i][1] + 1 : end_positions[i][1] + 1] = [1] * (end_positions[i][1] - start_positions[i][1])\n","    if TAGGING_SCHEME == \"BIOE\":\n","      current_propaganda_labels[start_positions[i][0]][start_positions[i][1]] = 2 # Begin label\n","      if start_positions[i][1] < end_positions[i][1]:\n","        current_propaganda_labels[start_positions[i][0]][start_positions[i][1] + 1 : end_positions[i][1]] = [1] * (end_positions[i][1] - start_positions[i][1] - 1)\n","        current_propaganda_labels[start_positions[i][0]][end_positions[i][1]] = 3 # End label\n","    else:\n","      current_propaganda_labels[start_positions[i][0]][start_positions[i][1] : end_positions[i][1] + 1] = [1] * (end_positions[i][1] + 1 - start_positions[i][1])\n","  \n","  num_sentences = len(current_sentence_tokens)\n","\n","  start_offset_list = get_list_from_dict(num_sentences, word_to_start_char_offset)\n","  end_offset_list = get_list_from_dict(num_sentences, word_to_end_char_offset)\n","  sentences = []\n","  for i in range(num_sentences):\n","    sentence = example_sentence()\n","    sentence.tokens = current_sentence_tokens[i]\n","    sentence.labels = current_propaganda_labels[i]\n","    sentence.article_index =  article_index\n","    sentence.index = i\n","    sentence.word_to_start_char_offset = start_offset_list[i]\n","    sentence.word_to_end_char_offset = end_offset_list[i]\n","    num_words = len(sentence.tokens)\n","    assert len(sentence.labels) == num_words\n","    assert len(sentence.word_to_start_char_offset) == num_words\n","    assert len(sentence.word_to_end_char_offset) == num_words\n","    sentences.append(sentence)\n","\n","  return current_sentence_tokens, current_propaganda_labels, (word_to_start_char_offset, word_to_end_char_offset), sentences"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OyvXc3mUr-Ln","colab_type":"code","colab":{}},"source":["def get_list_from_dict(num_sentences, word_offsets):\n","  li = []\n","  for _ in range(num_sentences):\n","    li.append([])\n","  for key in word_offsets:\n","    si = key[0]\n","    li[si].append(word_offsets[key])\n","\n","  return li"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGjKxzI72CE6","colab_type":"code","colab":{}},"source":["class BertExample:\n","  def __init__(self):\n","    self.add_cls_sep = True\n","    self.sentence_id = -1\n","    self.orig_to_tok_index = []\n","    self.tok_to_orig_index = []\n","    self.labels = None\n","    self.tokens_ids = []\n","    self.input_mask = []\n","  def __str__(self):\n","    print(\"sentence_id\", self.sentence_id)\n","    return \"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iuXFQT9rKaMk","colab_type":"code","outputId":"64ab4beb-1b7f-42d5-cf33-e08fd2cfe10c","executionInfo":{"status":"ok","timestamp":1582784756579,"user_tz":-330,"elapsed":3850,"user":{"displayName":"Paramansh Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mANS3lCUfabnCwNFz8A_Mb-FC_uqCl9Y4r-zhTcvA=s64","userId":"05094200950482431055"}},"colab":{"base_uri":"https://localhost:8080/","height":63}},"source":["from transformers import BertForTokenClassification\n","\n","def convert_sentence_to_input_feature(sentence, sentence_id, tokenizer, add_cls_sep=False, max_seq_len=256):\n","  bert_example = BertExample()\n","  bert_example.sentence_id = sentence_id\n","  bert_example.add_cls_sep = add_cls_sep\n","\n","  sentence_tokens = sentence.tokens\n","  sentence_labels = sentence.labels \n","\n","  tok_to_orig_index = []\n","  orig_to_tok_index = []\n","  all_doc_tokens = [] \n","  for (i, token) in enumerate(sentence_tokens):\n","    orig_to_tok_index.append(len(all_doc_tokens))\n","    sub_tokens = tokenizer.tokenize(token)\n","    for sub_token in sub_tokens:\n","      tok_to_orig_index.append(i)\n","      all_doc_tokens.append(sub_token)\n","  bert_example.tok_to_orig_index = tok_to_orig_index\n","  bert_example.orig_to_tok_index = orig_to_tok_index\n","\n","  bert_tokens = all_doc_tokens\n","  if add_cls_sep:\n","    bert_tokens = [\"[CLS]\"] + bert_tokens\n","    bert_tokens = bert_tokens + [\"[SEP]\"]\n","  \n","  tokens_ids = tokenizer.convert_tokens_to_ids(bert_tokens)\n","  input_mask = [1] * len(tokens_ids)\n","  while len(tokens_ids) < max_seq_len:\n","    tokens_ids.append(0)\n","    input_mask.append(0)\n","  # tokens_ids = pad_sequences(tokens_ids, maxlen=max_seq_len, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","  bert_example.tokens_ids = tokens_ids\n","  bert_example.input_mask = input_mask\n","  # bert_example.input_mask = [float(i>0) for i in token_ids]\n","\n","  if sentence_labels is None:\n","    return bert_example\n","  \n","\n","  labels = [0] * len(all_doc_tokens)\n","  for index, token in enumerate(all_doc_tokens):\n","    labels[index] = sentence_labels[tok_to_orig_index[index]]\n","  if add_cls_sep:\n","    labels = [0] + labels\n","    labels = labels + [0]\n","  # labels = pad_sequences(labels, maxlen=max_seq_len, truncating=\"post\", padding=\"post\", dtype=\"int\")\n","  while len(labels) < max_seq_len:\n","    labels.append(0)\n","  bert_example.labels = labels\n","\n","  return bert_example \n"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"1psU4R8EaoyA","colab_type":"code","colab":{}},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer\n","\n","def get_dataloader(examples, batch_size=8):\n","  inputs = torch.tensor([d.tokens_ids for d in examples])\n","  labels = torch.tensor([d.labels for d in examples])\n","  masks = torch.tensor([d.input_mask for d in examples])\n","  sentence_ids = torch.tensor([d.sentence_id for d in examples])\n","  tensor_data = TensorDataset(inputs, labels, masks, sentence_ids)\n","  dataloader = DataLoader(tensor_data, batch_size=BATCH_SIZE)\n","  return dataloader\n","\n","def get_data(articles, spans, indices):\n","  assert len(articles) == len(spans)    \n","  sentences = []\n","  for index in indices:\n","    article = articles[index]\n","    span = spans[index]\n","    _, _, _, cur_sentences = get_sentence_tokens_labels(article, span, index)\n","    sentences += cur_sentences\n","  print(len(sentences))\n","  print(max([len(s.tokens) for s in sentences]))\n","  bert_examples = []\n","  for i, sentence in enumerate(sentences):\n","    input_feature = convert_sentence_to_input_feature(sentence, i, tokenizer)\n","    bert_examples.append(input_feature)\n","  dataloader = get_dataloader(bert_examples, BATCH_SIZE)\n","  return dataloader, sentences, bert_examples"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgR-htBfZ6RX","colab_type":"code","colab":{}},"source":["def flat_accuracy(preds, labels):\n","  pred_flat = np.argmax(preds, axis=2).flatten()\n","  labels_flat = labels.flatten()\n","  return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ZtX5jmYUIMU","colab_type":"code","colab":{}},"source":["from tqdm import tqdm, trange\n","import time\n","import datetime\n","\n","def train(model, train_dataloader, eval_dataloader, epochs=5, save_model=False):\n","  max_grad_norm = 1.0\n","\n","  for _ in trange(epochs, desc=\"Epoch\"):\n","    # TRAIN loop\n","    model.train()\n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    for step, batch in enumerate(train_dataloader):\n","      # add batch to gpu\n","      batch = tuple(t.to(device) for t in batch)\n","      b_input_ids, b_labels, b_input_mask, b_ids = batch\n","      loss, _ = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","      loss.backward()\n","      tr_loss += loss.item()\n","      nb_tr_examples += b_input_ids.size(0)\n","      nb_tr_steps += 1\n","      torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","      optimizer.step()\n","      model.zero_grad()\n","    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","\n","    get_score(model, mode=\"train\")\n","\n","    model.eval()\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    predictions , true_labels = [], []\n","    for batch in eval_dataloader:\n","      batch = tuple(t.to(device) for t in batch)\n","      b_input_ids, b_labels, b_input_mask, b_ids = batch\n","      with torch.no_grad():\n","        tmp_eval_loss, _ = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","      # logits = logits[0]\n","      # logits = logits.detach().cpu().numpy()\n","      # label_ids = b_labels.to('cpu').numpy()\n","      # predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","      # true_labels.append(label_ids)\n","      \n","      # tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","      \n","      eval_loss += tmp_eval_loss.mean().item()\n","      # eval_accuracy += tmp_eval_accuracy\n","      \n","      nb_eval_examples += b_input_ids.size(0)\n","      nb_eval_steps += 1\n","    eval_loss = eval_loss/nb_eval_steps\n","    print(\"Validation loss: {}\".format(eval_loss))\n","\n","    get_score(model, mode=\"eval\")\n","    if save_model:\n","      model_name = 'model_' + str(datetime.datetime.now()) + '.pt'\n","      torch.save(model, os.path.join(model_dir, model_name))\n","      print(\"Model saved:\", model_name)\n","    print()\n","    time.sleep(1)\n","    # print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","    # pred_tags = [p_i for p in predictions for p_i in p]\n","    # valid_tags = [l_ii for l in true_labels for l_i in l for l_ii in l_i]\n","    # print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pOW1nliOdVKr","colab_type":"code","colab":{}},"source":["def get_model_predictions(model, dataloader):\n","  model.eval()\n","  predictions , true_labels, sentence_ids = [], [], []\n","  nb_eval_steps = 0\n","  for batch in dataloader:\n","    batch = tuple(t.to(device) for t in batch)\n","    b_input_ids, b_labels, b_input_mask, b_ids = batch  \n","    with torch.no_grad():\n","      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    logits = logits[0]\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    s_ids = b_ids.to('cpu').numpy()\n","    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","    # print(label_ids)\n","    true_labels.extend(label_ids)\n","    sentence_ids.extend(s_ids)\n","    nb_eval_steps += 1\n","  \n","  return predictions, true_labels, sentence_ids"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"974hWI4WB3lq","colab_type":"code","colab":{}},"source":["def merge_spans(current_spans):\n","  if not current_spans:\n","    return [] \n","  merged_spans = []\n","  li = current_spans[0][0]\n","  ri = current_spans[0][1]\n","  threshold = 2\n","  for i in range(len(current_spans) - 1):\n","    span = current_spans[i+1]\n","    if span[0] - ri < 2:\n","      ri = span[1]\n","      continue\n","    else:\n","      merged_spans.append((li, ri))\n","      li = span[0]\n","      ri = span[1]\n","  merged_spans.append((li, ri))\n","  return merged_spans"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZyV-2iMb8s7Y","colab_type":"code","colab":{}},"source":["from shutil import copyfile\n","def get_score(model, mode=None):\n","  predicted_spans = [[] for i in range(400)] # TODO 400 hardcoded\n","  \n","  def get_span_prediction(prediction_labels, sentence_index, sentences, bert_examples):\n","    index = sentence_index \n","    bert_example = bert_examples[index]\n","    mask = bert_example.input_mask\n","    pred_labels_masked = prediction_labels # need to change to predictions later\n","    pred_labels = []\n","    for i, m in enumerate(mask):\n","      if m > 0:\n","        pred_labels.append(pred_labels_masked[i])\n","    if bert_example.add_cls_sep:\n","      pred_labels.pop() # remove ['SEP'] label\n","      pred_labels.pop(0) # remove ['CLS'] label\n","\n","    sentence = sentences[index]\n","    sent_len = len(sentence.tokens)\n","    final_pred_labels = [0] * sent_len\n","    cur_map = bert_example.tok_to_orig_index\n","    for i, label in enumerate(pred_labels):\n","      final_pred_labels[cur_map[i]] |= label\n","    # assert final_pred_labels == sentence.labels\n","    \n","    word_start_index_map = sentence.word_to_start_char_offset\n","    word_end_index_map = sentence.word_to_end_char_offset\n","\n","    article_index = sentence.article_index\n","    for i, label in enumerate(final_pred_labels):\n","      if label:\n","        # print(word_start_index_map[i], word_end_index_map[i])\n","        predicted_spans[article_index].append((word_start_index_map[i], word_end_index_map[i]))\n","  \n","  if mode == \"train\":\n","    indices = train_indices\n","    predictions, true_labels, sentence_ids = get_model_predictions(model, train_dataloader)\n","    pred_sentences, pred_bert_examples = train_sentences, train_bert_examples\n","  elif mode == \"test\":\n","    predictions, true_labels , sentence_ids = get_model_predictions(model, test_dataloader)\n","    pred_sentences, pred_bert_examples = test_sentences, test_bert_examples\n","  else:\n","    indices = eval_indices\n","    predictions, true_labels, sentence_ids = get_model_predictions(model, eval_dataloader)\n","    pred_sentences, pred_bert_examples = eval_sentences, eval_bert_examples\n","\n","  merged_predicted_spans = []\n","  # TODO sorting of spans???? may not be in order??\n","  for ii, _ in enumerate(predictions):\n","    get_span_prediction(predictions[ii], sentence_ids[ii], pred_sentences, pred_bert_examples)\n","  for span in predicted_spans:\n","    merged_predicted_spans.append(merge_spans(span))\n","  if mode == \"test\":\n","    return merged_predicted_spans \n","  if not os.path.isdir(\"predictions\"):\n","    os.mkdir(\"predictions\")\n","  copyfile(\"gdrive/My Drive/propaganda_detection/tools/task-SI_scorer.py\", \"predictions/task-SI_scorer.py\")\n","  with open(\"predictions/predictions.tsv\", 'w') as fp:\n","    for index in indices:\n","      filename = \"article\" + article_ids[index] + \".task1-SI.labels\"\n","      copyfile(os.path.join(data_dir, \"train-labels-task1-span-identification/\" + filename), \"predictions/\" + filename)\n","      for ii in merged_predicted_spans[index]:\n","        fp.write(article_ids[index] + \"\\t\" + str(ii[0]) + \"\\t\" + str(ii[1]) + \"\\n\")\n","\n","  !python3 predictions/task-SI_scorer.py -s predictions/predictions.tsv -r predictions/ -m\n","\n","  for index in indices:\n","    filename = \"article\" + article_ids[index] + \".task1-SI.labels\"\n","    os.remove(\"predictions/\" + filename)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xb9uHCd8gKVa","colab":{}},"source":["from transformers import GPT2Tokenizer, GPT2Model, GPT2PreTrainedModel\n","import torch\n","import torch.nn as nn\n","\n","class GPT2ForTokenClassification(GPT2PreTrainedModel):\n","  def __init__(self,config):\n","    super().__init__(config)\n","    self.num_labels = config.num_labels\n","\n","    self.gpt2 = GPT2Model(config)\n","    self.dropout = nn.Dropout(config.resid_pdrop)\n","    self.classifier = nn.Linear(config.n_embd, config.num_labels)\n","    self.init_weights()\n","  \n","  def forward(self,input_ids=None,\n","              attention_mask=None,\n","              token_type_ids = None,\n","              position_ids = None,\n","              head_mask = None,\n","              input_embeds = None,\n","              labels = None,\n","              ):\n","    output = self.gpt2(\n","        input_ids,\n","        attention_mask = attention_mask,\n","        token_type_ids = token_type_ids,\n","        position_ids = position_ids,\n","        head_mask = head_mask,\n","        inputs_embeds = input_embeds, \n","    )\n","\n","    sequence_output = output[0]\n","    sequence_output = self.dropout(sequence_output)\n","    logits = self.classifier(sequence_output)\n","    outputs = (logits,) + output[2:]\n","    if labels is not None:\n","      loss_fct =  torch.nn.CrossEntropyLoss()\n","\n","      if attention_mask is not None:\n","        active_loss = attention_mask.view(-1)==1\n","        active_logits = logits.view(-1, self.num_labels)[active_loss]\n","        active_labels = labels.view(-1)[active_loss]\n","        loss = loss_fct(active_logits, active_labels)\n","      else:\n","        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","      outputs = (loss,) + outputs\n","    \n","    return outputs\n","        \n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pwrmRjA6-CXO","colab":{}},"source":["articles, article_ids = read_articles('train-articles')\n","spans = read_spans()\n","# TAGGING_SCHEME = \"PN\" # Positive Negative\n","# TAGGING_SCHEME = \"PN\"\n","NUM_ARTICLES = len(articles)\n","# NUM_ARTICLES = 140\n","articles = articles[0:NUM_ARTICLES]\n","spans = spans[0:NUM_ARTICLES]\n","BATCH_SIZE=16\n","np.random.seed(245)\n","indices = np.arange(NUM_ARTICLES)\n","np.random.shuffle(indices)\n","train_indices = indices[:int(0.9 * NUM_ARTICLES)]\n","eval_indices = indices[int(0.9 * NUM_ARTICLES):]\n","\n","# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', lower_case=True)\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","train_dataloader, train_sentences, train_bert_examples = get_data(articles, spans, train_indices)\n","eval_dataloader, eval_sentences, eval_bert_examples = get_data(articles, spans, eval_indices)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"auvkz5B7tIjp","colab_type":"code","outputId":"ba160fd1-d6fc-4d41-c1fb-b61441636af3","executionInfo":{"status":"ok","timestamp":1582736471002,"user_tz":-330,"elapsed":1994610,"user":{"displayName":"Paramansh Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mANS3lCUfabnCwNFz8A_Mb-FC_uqCl9Y4r-zhTcvA=s64","userId":"05094200950482431055"}},"colab":{"base_uri":"https://localhost:8080/","height":615}},"source":["num_labels = 2 + int(TAGGING_SCHEME == \"BIO\") + 2 * int(TAGGING_SCHEME == \"BIOE\")\n","# model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n","model = GPT2ForTokenClassification.from_pretrained('gpt2', num_labels=num_labels)\n","model.cuda()\n","from torch.optim import Adam\n","FULL_FINETUNING = True \n","if FULL_FINETUNING:\n","  param_optimizer = list(model.named_parameters())\n","  no_decay = ['bias', 'gamma', 'beta']\n","  optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","      'weight_decay_rate': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","      'weight_decay_rate': 0.0}\n","  ]\n","else:\n","  param_optimizer = list(model.classifier.named_parameters()) \n","  optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","optimizer = Adam(optimizer_grouped_parameters, lr=3e-5) # lr 3e-5\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","train(model, train_dataloader, eval_dataloader, epochs=3, save_model=(NUM_ARTICLES >= 1150))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.42194473459429965\n","2020-02-26 16:38:05,472 - INFO - Checking user submitted file\n","2020-02-26 16:38:05,571 - INFO - Scoring the submission with precision and recall method\n","2020-02-26 16:38:05,571 - INFO - Precision=0.000000/0=0.000000\tRecall=0.000000/4790=0.000000\n","2020-02-26 16:38:05,571 - INFO - F1=0.000000\n","Validation loss: 0.4575718798173162\n","2020-02-26 16:38:57,596 - INFO - Checking user submitted file\n","2020-02-26 16:38:57,609 - INFO - Scoring the submission with precision and recall method\n","2020-02-26 16:38:57,609 - INFO - Precision=0.000000/0=0.000000\tRecall=0.000000/607=0.000000\n","2020-02-26 16:38:57,609 - INFO - F1=0.000000\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [10:59<21:59, 659.65s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.3784132152111433\n","2020-02-26 16:49:05,448 - INFO - Checking user submitted file\n","2020-02-26 16:49:05,613 - INFO - Scoring the submission with precision and recall method\n","2020-02-26 16:49:07,394 - INFO - Precision=2906.021059/6212=0.467808\tRecall=908.257085/4790=0.189615\n","2020-02-26 16:49:07,394 - INFO - F1=0.269852\n","Validation loss: 0.48250269549540603\n","2020-02-26 16:49:59,506 - INFO - Checking user submitted file\n","2020-02-26 16:49:59,524 - INFO - Scoring the submission with precision and recall method\n","2020-02-26 16:49:59,722 - INFO - Precision=207.128302/644=0.321628\tRecall=63.761048/607=0.105043\n","2020-02-26 16:49:59,722 - INFO - F1=0.158364\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [22:01<11:00, 660.32s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.34340670229857184\n","2020-02-26 17:00:07,352 - INFO - Checking user submitted file\n","2020-02-26 17:00:07,611 - INFO - Scoring the submission with precision and recall method\n","2020-02-26 17:00:10,782 - INFO - Precision=4138.413531/10008=0.413511\tRecall=1904.644010/4790=0.397629\n","2020-02-26 17:00:10,782 - INFO - F1=0.405414\n","Validation loss: 0.5041471553536562\n","2020-02-26 17:01:04,076 - INFO - Checking user submitted file\n","2020-02-26 17:01:04,111 - INFO - Scoring the submission with precision and recall method\n","2020-02-26 17:01:04,485 - INFO - Precision=371.065632/1243=0.298524\tRecall=136.864170/607=0.225476\n","2020-02-26 17:01:04,485 - INFO - F1=0.256909\n","\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch: 100%|██████████| 3/3 [33:09<00:00, 662.53s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wPBCR48PGVu-","colab_type":"code","outputId":"da69f3e1-c0b7-4b72-8074-b049847e1d0a","executionInfo":{"status":"error","timestamp":1582740206828,"user_tz":-330,"elapsed":1953,"user":{"displayName":"Paramansh Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mANS3lCUfabnCwNFz8A_Mb-FC_uqCl9Y4r-zhTcvA=s64","userId":"05094200950482431055"}},"colab":{"base_uri":"https://localhost:8080/","height":169}},"source":["train(model, train_dataloader, eval_dataloader, epochs=3, save_model=(NUM_ARTICLES >= 150))"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-99f4c31caec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_ARTICLES\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"]}]},{"cell_type":"code","metadata":{"id":"AUmj8j4M878b","colab_type":"code","outputId":"3528e9b7-452f-4e0b-9fc7-2b4851437105","executionInfo":{"status":"ok","timestamp":1582121749345,"user_tz":-330,"elapsed":340298,"user":{"displayName":"Paramansh Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mANS3lCUfabnCwNFz8A_Mb-FC_uqCl9Y4r-zhTcvA=s64","userId":"05094200950482431055"}},"colab":{"base_uri":"https://localhost:8080/","height":181}},"source":[" # model = torch.load(os.path.join(data_dir, 'model.pt'))\n","get_score(model, mode=\"train\")\n","print()\n","get_score(model, mode=\"eval\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020-02-19 14:15:10,024 - INFO - Checking user submitted file\n","2020-02-19 14:15:10,208 - INFO - Scoring the submission with precision and recall method\n","2020-02-19 14:15:12,003 - INFO - Precision=3480.549402/4050=0.859395\tRecall=2979.603441/4790=0.622047\n","2020-02-19 14:15:12,003 - INFO - F1=0.721707\n","\n","2020-02-19 14:15:47,250 - INFO - Checking user submitted file\n","2020-02-19 14:15:47,267 - INFO - Scoring the submission with precision and recall method\n","2020-02-19 14:15:47,402 - INFO - Precision=230.024532/394=0.583819\tRecall=166.455081/607=0.274226\n","2020-02-19 14:15:47,402 - INFO - F1=0.373170\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AVKzjMZOMoFV","colab_type":"code","outputId":"53af129c-7ebb-4a75-c41d-bce609eb4f82","executionInfo":{"status":"ok","timestamp":1582643995251,"user_tz":-330,"elapsed":53820,"user":{"displayName":"Paramansh Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mANS3lCUfabnCwNFz8A_Mb-FC_uqCl9Y4r-zhTcvA=s64","userId":"05094200950482431055"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["ind = 32\n","# model = torch.load(os.path.join(model_dir, 'model_300_42.pt'))\n","\n","test_articles, test_article_ids = read_articles('dev-articles')\n","# test_articles = [articles[ind]]\n","test_spans = [[]] * len(test_articles)\n","\n","test_dataloader, test_sentences, test_bert_examples = get_data(test_articles, test_spans, indices=np.arange(len(test_articles)))\n","sps = get_score(model, mode=\"test\")\n","# print_spans(articles[ind], sps[0])\n","# print('--' * 50)\n","# print_spans(articles[ind], spans[ind])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["3177\n","103\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gf_VICv6u5-q","colab_type":"code","colab":{}},"source":["from google.colab import files\n","with open('predictions/dev_predictions.txt', 'w') as fp:\n","  for index in range(len(test_articles)):\n","    for ii in sps[index]:\n","      fp.write(test_article_ids[index] + \"\\t\" + str(ii[0]) + \"\\t\" + str(ii[1]) + \"\\n\")\n","files.download('predictions/dev_predictions.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9HK4sY9KENbV","colab_type":"code","colab":{}},"source":["# torch.save(model, os.path.join(model_dir, 'model_300_42.pt'))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-98Mlmdk4S7n","colab_type":"code","outputId":"f83df49f-8d34-4188-fff7-a7cb743f1dfd","executionInfo":{"status":"ok","timestamp":1580328283170,"user_tz":-330,"elapsed":1019,"user":{"displayName":"Paramansh Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mANS3lCUfabnCwNFz8A_Mb-FC_uqCl9Y4r-zhTcvA=s64","userId":"05094200950482431055"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for index in eval_indices:\n","  print(\"article index:\",index)\n","  span = merged_predicted_spans[index]\n","  print_spans(articles[index], span)\n","  print()\n","  print_spans(articles[index], spans[index])\n","  print(\"--\" * 50)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["article index: 107\n","“sort of in a panic”\n","that someone might “break into his apartment” looking for it, like “that Watergate crap.”\n","it,”\n","black-owned businesses.\n","institutions\n","“spiritual advisor”\n","Reading Obama’s 1995 memoir, you might almost get the impression that after a prudent first term, during his second he might side with, I dunno, Black Lives Matter and encourage a wave of black rage and police retreat that drove up the death toll from murder by 20% in his last two years in office, an incremental death toll a little bigger than the U.S. combat death toll from the equally stupid Iraq War.\n","\n","\n","spiritual advisor\n","sort of in a panic\n","black\n","shaking down white institutions\n","Reading Obama’s 1995 memoir, you might almost get the impression that after a prudent first term, during his second he might side with, I dunno, Black Lives Matter and encourage a wave of black rage and police retreat that drove up the death toll from murder by 20% in his last two years in office, an incremental death toll a little bigger than the U.S. combat death toll from the equally stupid Iraq War\n","was afraid that someone might “break into his apartment” looking for it\n","crap\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 123\n","Hitler-Loving Anti-Semite\n","The media is very interested in racism.\n","“outstanding human being”\n","white people “deserve to die”\n","Adolf Hitler\n","a “very great man.”\n","him as an outstanding human being who commands a following of individuals who are learned and articulate and he plays a big role in the lives of thousands and thousands and thousands and thousands of people,”\n","many people in politics have a history of inflammatory comments.\n","Farrakhan had praised Hitler and declared that the Jews, \"can't say 'Never Again' to God, because when he puts you in the ovens, you're there forever.”\n","\"Here come the Jews.\n","They don't like Farrakhan, so they call me Hitler.\n","Well, that's a good name.\n","Hitler was a very great man.\"\n","Would a Republican get a pass on meeting with a racist hate group leader?\n","Obviously not.\n","Don't expect it to condemn Rep. Davis' comments.\n","He was just saying what most of them think.\n","The media is happy to talk about anti-Semitism.\n","It just won't address left-wing anti-Semitism and racism.\n","\n","\n","I regard him as an outstanding human being who commands a following of individuals who are learned and articulate and he plays a big role in the lives of thousands and thousands and thousands and thousands of people,”\n","Would a Republican get a pass on meeting with a racist hate group leader?\n","Obviously not\n","Don't expect it to condemn Rep. Davis' comments.\n","He was just saying what most of them think\n","Farrakhan had praised Hitler and declared that the Jews, \"can't say 'Never Again' to God, because when he puts you in the ovens, you're there forever.”\n","\"Here come the Jews.\n","They don't like Farrakhan, so they call me Hitler.\n","Well, that's a good name.\n","Hitler was a very great man.\"\n","Loving Anti-Semite\n","The media is very interested in racism\n","The media is happy to talk about anti-Semitism\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 29\n","burning\n","There are obviously certain things that the FBI and other investigators seem intent on covering up.\n","so why change the narrative to make it even more unbelievable?\n","What exactly do they want us to believe?\n","\n","\n","What exactly do they want us to believe?\n","There are obviously certain things that the FBI and other investigators seem intent on covering up\n","so why change the narrative to make it even more unbelievable?\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 56\n","striking example\n","a widespread cover-up\n","stunning tower audio once\n","complete nonsense\n","authorities are actually actively covering up what really happened on\n","night.\n","flag,\n","the American people are clearly not being told the truth.\n","\n","\n","widespread cover-up\n","the American people are clearly not being told the truth\n","complete nonsense\n","authorities are actually actively covering up what really happened\n","striking example\n","stunning tower audio\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 63\n","Deadly Plague Could MUTATE And Become Untreatable As It Spreads Globally\n","country’s worst outbreak in 50 years.\n","the deadly bacteria could mutate and become untreatable as it spreads across the globe.\n","high likelihood that this disease could spread globally\n","infected traveler\n","America, health officials\n","warning\n","killed more than\n","Madagascar could mutate and become untreatable.\n","while it would be rather easy for an advanced country to contain the disease in its current form, he fears that it could evolve into something far more dangerous.\n","to the Ebola outbreak.\n","“As with any disease, it’s a real worry that it mutates and become untreatable,”\n","devastated Europe’s population in\n","Airborne infections are difficult at best to control.\n","Ten African nations have already been put on alert that the plague could easily spread to their region of the globe.\n","spread.\n","\n","\n","Plague Could MUTATE And Become Untreatable As It Spreads Globally\n","the deadly bacteria could mutate and become untreatable\n","could mutate and become untreatable\n","he fears that it could evolve into something far more dangerous\n","it’s a real worry that it mutates and become untreatable,”\n","devastated\n","difficult at best\n","already been put on alert\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 69\n","It Is ‘Inevitable’ The Plague Becomes Resistant To Drugs\n","black death,\n","Officials warn that it’s inevitable that this bacterial infection that’s infected over 2000 people will become resistant to antibiotics.\n","antibiotics resistance is inevitable and making this disease much more terrifying.\n","Once the bacteria is resistant,\n","Madagascar’s healthcare system will be unable to cope if the deadly plague outbreak continues to escalate,\n","fears hospitals will be unable to\n","“worst outbreak in 50 years.”\n","black death\n","Although the plague is responding well to antibiotics\n","now, drug resistance is also an increasing concern amongst experts who predict it will vastly accelerate the disease’s death toll.\n","“Fortunately in [the] plague, it has not developed much antibiotic resistance.\n","If that kicks in, the plague will be far, far scarier.\n","patients, antibiotic resistance is more or less inevitable.”\n","country\n","struggle\n","if cases continue to spiral.\n","experts\n","fear the healthcare system is on\n","brink of being overwhelmed.\n","it will be all but impossible to control and the health care system would certainly be unable to handle the outbreak at that point, making a global pandemic much more likely.\n","\n","\n","black death\n","It Is ‘Inevitable’ The Plague Becomes Resistant To Drugs\n","it’s inevitable that this bacterial infection\n","antibiotics resistance is inevitable\n","Madagascar’s healthcare system will be unable to cope if the deadly plague outbreak continues to escalate\n","the “worst outbreak in 50 years\n","black death\n","drug resistance is also an increasing concern\n","antibiotic resistance is more or less inevitable\n","making this disease much more terrifying\n","If that kicks in, the plague will be far, far scarier\n","making a global pandemic much more likely\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 55\n","brave defenders of the status quo\n","confusion”\n","“To teach with such an intentional lack of clarity inevitably risks sinning against the Holy Spirit, the Spirit of truth.”\n","many bishops don’t speak out publicly for fear they will be “marginalized or worse.”\n","given the boot.\n","the most astonishing aspect\n","this little incident\n","completely blind\n","And it looks extremely bad.\n","irony\n","cheap and vulgar ritual humiliation\n","bully-boy Church\n","midget bishops and minicardinals compete\n","each other\n","episcopal bullies.\n","absolutely\n","Appointing bishops who “scandalize” believers with dubious “teaching\n","practice.”\n","impression they’ll be “marginalized or worse” if they speak out.\n","supreme shepherd.”\n","“I have done what I believe God wanted me to do,[1]”\n","has bolstered my own “Great Clarifier” theory,\n","“just how weak is the faith of many within the Church,”\n","harmful theological\n","pastoral views.”\n","he was indeed got rid of.\n","complete inability\n","be approved of by the “cool kids” in the Vatican,\n","his own theological brilliance\n","Signaling furiously with the trendy FrancisChurch buzzwords and even trendier blithering incoherence,\n","Presumably because adultery itself is no longer sinful.\n","evangelization”\n","adultery\n","sacrilege\n","poor, poor woman\n","been abandoned by the first husband,\n","“finds no other way out than to\n","oneself to a kind-hearted person,” … with whom, I guess, she has also no choice but to have sexual relations.\n","Because of kind-heartedness.\n","relativistic Zeitgeist,”\n","There\n","“different levels” of gravity,\n","has\n","“falsely elevated to the rank of a decisive question of Catholicism and a measure of ideological comparison in order to decide whether one is conservative or liberal, in favor or against the pope.”\n","even threatening to go into schism if they didn’t get their way[2].\n","lionized by\n","howling tantrums\n","\n","\n","I have done what I believe God wanted me to do,[\n","bolstered my own “Great Clarifier” theory\n","the poor, poor woman\n","brave defenders of the status quo\n","cheap and vulgar ritual humiliation\n","bully-boy Church\n","midget bishops and minicardinals\n","episcopal bullies\n","who “scandalize” believers\n","supreme shepherd.”\n","got rid of\n","the “cool kids” in the Vatican\n","Signaling furiously\n","with whom, I guess, she has also no choice but to have sexual relations.\n","Because of kind-heartedness\n","falsely elevated to the rank of a decisive question of Catholicism and a measure of ideological comparison in order to decide whether one is conservative or liberal, in favor or against the pope.”\n","howling tantrums\n","To teach with such an intentional lack of clarity inevitably risks sinning against the Holy Spirit, the Spirit of truth\n","don’t speak out publicly for fear they will be “marginalized or worse\n","given the boot\n","little incident\n","extremely bad\n","they’ll be “marginalized or worse” if they speak out\n","blithering incoherence\n","Presumably because adultery itself is no longer sinful\n","relativistic Zeitgeist\n","even threatening to go into schism if they didn’t get their way\n","lionized\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 18\n","Barack Obama's reckless appeasement of the Iranian regime has enabled its rise as a hegemonic threat\n","as a threat\n","Obama turned his back on millions of dissidents\n","Obama precipitously removed the remaining U.S. combat troops from Iraq, giving rise to ISIS’s re-emergence in Iraq from its bases in Syria.\n","terrorists, turning Iraq into a virtual vassal state of the largest state sponsor of terrorism in the process.\n","Iran legitimized Iran's path to eventually becoming a nuclear-armed state,\n","“It’s a new monster.”\n","Hezbollah has American blood on its hands,\n","milquetoast\n","could actually accelerate the cycle of violence and perpetuate conflict rather than get us to a sustainable outcome.”\n","Whether the Trump administration follows through remains to be seen.\n","\n","\n","Barack Obama's reckless appeasement of the Iranian regime has enabled its rise as a hegemonic threat\n","disastrous\n","“It’s a new monster.”\n","milquetoast\n","turned his back on millions of dissidents\n","giving rise to ISIS’s re-emergence in Iraq\n","turning Iraq into a virtual vassal state of the largest state sponsor of terrorism\n","legitimized Iran's path to eventually becoming a nuclear-armed state\n","American blood\n","could actually accelerate the cycle of violence and perpetuate conflict rather than get us to a sustainable outcome\n","Whether the Trump administration follows through remains to be seen\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 0\n","plague epidemic\n","\"The next transmission could be more pronounced or stronger,\"\n","\"plague in Madagascar behaved in a very, very different way this year.\"\n","He also pointed to the presence of the pneumonic version, which spreads more easily and is more virulent, in the latest outbreak.\n","warned that the danger was not over.\n","\"when (the plague) comes again it starts from more stock, and the magnitude in the next transmission could be higher than the one that we saw,\"\n","it could even spill over into neighbouring countries and beyond,\"\n","\n","\n","The next transmission could be more pronounced or stronger\n","when (the plague) comes again it starts from more stock, and the magnitude in the next transmission could be higher than the one that we saw\n","appeared\n","a very, very different\n","He also pointed to the presence of the pneumonic version, which spreads more easily and is more virulent, in the latest outbreak\n","but warned that the danger was not over\n","it could even spill over into neighbouring countries and beyond\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 100\n","gross negligence\n","a massive cover-up of the truth, the F.B.I.\n","and LVMPD have failed to record the death of at least one shooting victim\n","that law enforcement investigators have never asked to see the footage.\n","“placed” outside of\n","Who was this victim?\n","Will the F.B.I.\n","and LVMPD please explain what in the Hell is going on here?\n","How was this victim overlooked by investigators but known by Intellihub?\n","Who was this victim?\n","Why were people getting haircuts at the crime scene?\n","How many more victims went ignored by investigators?\n","\n","\n","gross negligence\n","How was this victim overlooked by investigators but known by Intellihub?\n","Who was this victim?\n","a massive cover-up of the truth, the F.B.I.\n","and LVMPD\n","law enforcement investigators have never asked to see the footage\n","placed”\n","Will the F.B.I.\n","and LVMPD please explain what in the Hell is going on here?\n","How many more victims went ignored by investigators?\n","botched investigation\n","were people getting haircuts at the crime scene?\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 73\n","How Do You Like Paying For Sexual Harassment Settlements from Your Congress, America?\n","How Do You Like Paying For Sexual Harassment Settlements from Your Congress, America?\n","Thank you!\n","they aren’t pretty.\n","That money has come from the taxpayers of course.\n","epidemic of sexual harassment\n","If such a law was instituted, I guarantee you that sexual harassment on Capitol Hill would come to a screeching halt.\n","For many in Congress, having so many attractive young women around is one of the key benefits of the job.\n","the men “have no self-control.” “Amongst\n","with the worst reputations.\n","“sex trade on Capitol Hill.”\n","It isn’t going to stop until we boot out the corrupt career politicians that are engaging in this type of behavior.\n","“grassroots deplorables”\n","The American people have a right to know,\n","\n","\n","the epidemic of sexual harassment\n","How Do You Like Paying For Sexual Harassment Settlements from Your Congress, America?\n","How Do You Like Paying For Sexual Harassment Settlements from Your Congress, America?\n","they aren’t pretty\n","That money has come from the taxpayers of course\n","If such a law was instituted, I guarantee you that sexual harassment on Capitol Hill would come to a screeching halt\n","For many in Congress, having so many attractive young women around is one of the key benefits of the job\n","the men “have no self-control.”\n","the lawmakers with the worst reputations\n","sex trade on Capitol Hill.”\n","It isn’t going to stop until we boot out the corrupt career politicians that are engaging in this type of behavior\n","grassroots deplorables”\n","The American people have a right to know\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 117\n","the deadliest mass shooting in modern U.S. history\n","\n","\n","the deadliest mass shooting in modern U.S. history\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 137\n","Sociology helped generate a whole range of fake new academic subjects while corrupting existing ones into a toxic stew of racism and meaningless jargon.\n","absolute ignorance.\n","a perfect storm of sociology stupidity.\n","Sociology.\n","You really don't need to know anything.\n","The professor, who has a PhD in philosophy...\n","Of course she does.\n","the age of, \"My truth\".\n","\n","\n","a perfect storm of sociology stupidity\n","Sociology.\n","You really don't need to know anything\n","my truth\n","Sociology helped generate a whole range of fake new academic subjects while corrupting existing ones into a toxic stew of racism and meaningless jargon\n","absolute ignorance\n","shocked\n","The professor, who has a PhD in philosophy...\n","Of course she does\n","the age of, \"My truth\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 103\n","\n","\n","\n","----------------------------------------------------------------------------------------------------\n","article index: 46\n","Are\n","‘Dancing With Corpses’\n","“dancing with corpses.”\n","The local tradition is said to be one of the major causes of the spread of this disease.\n","Health officials suspect it’s no coincidence that the outbreak coincides with the time of year when families customarily exhume the remains of dead relatives,\n","ritual,\n","the bacteria can still be transmitted and contaminate whoever handles the body,”\n","Some locals believe the whole plague is some kind of a government conspiracy,\n","The plague is a lie,”\n","cash-strapped government is just exaggerating the problem to get money ahead of an election next year.\n","And it isn’t that anyone is putting this past a government to lie for profits,\n","\n","\n","People Are Literally ‘Dancing With Corpses’\n","dancing with corpses.”\n","The local tradition is said to be one of the major causes of the spread of this disease\n","Some locals believe the whole plague is some kind of a government conspiracy\n","Health officials suspect it’s no coincidence that the outbreak coincides with the time of year when families customarily exhume the remains of dead relatives\n","The plague is a lie\n","government is just exaggerating the problem to get money ahead of an election next year\n","it isn’t that anyone is putting this past a government to lie for profits\n","the bacteria can still be transmitted and contaminate whoever handles the body\n","cash-strapped\n","\n","----------------------------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6KYdIrkD8op0","colab_type":"code","colab":{}},"source":["# from shutil import copyfile\n","# indices = eval_indices\n","# predictions, true_labels, sentence_ids = get_predictions(model, eval_dataloader)\n","# predicted_spans = [[] for i in range(NUM_ARTICLES)]\n","# merged_predicted_spans = []\n","# for ii, _ in enumerate(predictions):\n","#   evaluate_prediction(predictions[ii], sentence_ids[ii], eval_sentences, eval_bert_examples)\n","# for span in predicted_spans:\n","#   merged_predicted_spans.append(merge_spans(span))  \n","\n","# from shutil import copyfile\n","# if not os.path.isdir(\"predictions\"):\n","#   os.mkdir(\"predictions\")\n","# copyfile(\"gdrive/My Drive/propaganda_detection/tools/task-SI_scorer.py\", \"predictions/task-SI_scorer.py\")\n","\n","# with open(\"predictions/predictions.tsv\", 'w') as fp:\n","#   for index in indices:\n","#     filename = \"article\" + article_ids[index] + \".task1-SI.labels\"\n","#     copyfile(os.path.join(data_dir, \"train-labels-task1-span-identification/\" + filename), \"predictions/\" + filename)\n","#     for ii in merged_predicted_spans[index]:\n","#       fp.write(article_ids[index] + \"\\t\" + str(ii[0]) + \"\\t\" + str(ii[1]) + \"\\n\")\n","\n","# !python3 predictions/task-SI_scorer.py -s predictions/predictions.tsv -r predictions/ -m\n","\n","# for index in indices:\n","#   filename = \"article\" + article_ids[index] + \".task1-SI.labels\"\n","#   os.remove(\"predictions/\" + filename)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DRgZVwMAPrK-","colab_type":"text"},"source":["#GPT-2"]},{"cell_type":"code","metadata":{"id":"WU8kNdsqPtT5","colab_type":"code","colab":{}},"source":["from transformers import GPT2Tokenizer, GPT2Model, GPT2PreTrainedModel\n","import torch\n","import torch.nn as nn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y6Ch6xAjkSLl","colab_type":"code","colab":{}},"source":["# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","# text = \"What is the fastest runningggggggggg car in the\"\n","# indexed_tokens = tokenizer.encode(text)\n","# print(indexed_tokens)\n","# print(tokenizer.decode([2491]))\n","# tokens_tensor = torch.tensor([indexed_tokens])\n","# print(tokens_tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0h1KFoFqlxJB","colab_type":"code","colab":{}},"source":["class GPT2ForTokenClassification(GPT2PreTrainedModel):\n","  def __init__(self,config):\n","    super().__init__(config)\n","    self.num_labels = config.num_labels\n","\n","    self.gpt2 = GPT2Model(config)\n","    self.dropout = nn.Dropout(config.resid_pdrop)\n","    self.classifier = nn.Linear(config.n_embd, config.num_labels)\n","    self.init_weights()\n","  \n","  def forward(self,input_ids=None,\n","              attention_mask=None,\n","              token_type_ids = None,\n","              position_ids = None,\n","              head_mask = None,\n","              input_embeds = None,\n","              labels = None,\n","              ):\n","    output = self.gpt2(\n","        input_ids,\n","        attention_mask = attention_mask,\n","        token_type_ids = token_type_ids,\n","        position_ids = position_ids,\n","        head_mask = head_mask,\n","        inputs_embeds = input_embeds, \n","    )\n","\n","    sequence_output = output[0]\n","    sequence_output = self.dropout(sequence_output)\n","    logits = self.classifier(sequence_output)\n","    outputs = (logits,) + output[2:]\n","    if labels is not None:\n","      loss_fct =  torch.nn.CrossEntropyLoss()\n","\n","      if attention_mask is not None:\n","        active_loss = attention_mask.view(-1)==1\n","        active_logits = logits.view(-1, self.num_labels)[active_loss]\n","        active_labels = labels.view(-1)[active_loss]\n","        loss = loss_fct(active_logits, active_labels)\n","      else:\n","        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","      outputs = (loss,) + outputs\n","    \n","    return outputs\n","        \n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"muelS5vqED1X","colab_type":"code","colab":{}},"source":["model = GPT2ForTokenClassification.from_pretrained('gpt2', num_labels=2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TjaffIXKE48W","colab_type":"code","outputId":"7c7ce076-2d9d-4dae-9a1e-e0c2bc8e4781","executionInfo":{"status":"ok","timestamp":1580213765612,"user_tz":-330,"elapsed":1031,"user":{"displayName":"Subham Kumar","photoUrl":"","userId":"15408684190797250143"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = model.cuda()\n","print(model)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["GPT2ForTokenClassification(\n","  (gpt2): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UN17N0lRFB2H","colab_type":"code","colab":{}},"source":["gpt2tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"],"execution_count":0,"outputs":[]}]}