{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline Technique Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WkH2d0SLPeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49fpc1iF6jcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDlMmtRU1MDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import pprint\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyXwpKlEWE4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "home_dir = \"gdrive/My Drive/propaganda_detection\"\n",
        "data_dir = os.path.join(home_dir, \"datasets\")\n",
        "model_dir = os.path.join(home_dir, \"model_dir\")\n",
        "if not os.path.isdir(model_dir):\n",
        "  os.mkdir(model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H93QqgFyxL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read training articles\n",
        "def read_articles(dir_name):\n",
        "  articles = []\n",
        "  train_dir = os.path.join(data_dir, dir_name)\n",
        "  for filename in sorted(os.listdir(train_dir)):\n",
        "    myfile = open(os.path.join(train_dir, filename))\n",
        "    article = myfile.read()\n",
        "    articles.append(article)\n",
        "    myfile.close()\n",
        "  article_ids = []\n",
        "  for filename in sorted(os.listdir(train_dir)):\n",
        "    article_ids.append(int(filename[7:-4]))\n",
        "  return articles, article_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfBpePjiH_-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read training span labels \n",
        "def read_spans(mode=None):\n",
        "  spans = []\n",
        "  techniques = []\n",
        "  if mode == \"test\":\n",
        "    label_dir = os.path.join(data_dir, \"dev-task-TC-template.out\")\n",
        "  else:\n",
        "    label_dir = os.path.join(data_dir, \"train-labels-task2-technique-classification\")\n",
        "  for filename in sorted(os.listdir(label_dir)):\n",
        "    myfile = open(os.path.join(label_dir, filename))\n",
        "    tsvreader = csv.reader(myfile, delimiter=\"\\t\")\n",
        "    span = []\n",
        "    technique = []\n",
        "    for row in tsvreader:\n",
        "      span.append((int(row[2]), int(row[3])))\n",
        "      if mode == \"test\":\n",
        "        technique.append(\"Slogans\") # DUMMY\n",
        "      else:\n",
        "        technique.append(row[1])\n",
        "    myfile.close()\n",
        "    spans.append(span)\n",
        "    techniques.append(technique)\n",
        "  return spans, techniques"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9FjNDEZ4tnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read training span labels \n",
        "def read_test_spans(mode=None):\n",
        "  spans = []\n",
        "  techniques = []\n",
        "  indices = []\n",
        "  if mode == 'test':\n",
        "    label_file = os.path.join(data_dir, \"test-TC/test-task-TC-template.out\")\n",
        "  else:  \n",
        "    label_file = os.path.join(data_dir, \"dev-task-TC-template.out\")\n",
        "  myfile = open(label_file)\n",
        "  prev_index = -1\n",
        "  tsvreader = csv.reader(myfile, delimiter=\"\\t\")\n",
        "\n",
        "  span = []\n",
        "  technique = []\n",
        "  for row in tsvreader:\n",
        "    article_index = int(row[0])\n",
        "    if article_index != prev_index:\n",
        "      if prev_index != -1:\n",
        "        spans.append(span)\n",
        "        techniques.append(technique)\n",
        "      span = []\n",
        "      technique = []\n",
        "      span.append((int(row[2]), int(row[3])))\n",
        "      technique.append(\"Slogans\")\n",
        "      indices.append(article_index)\n",
        "      prev_index = article_index\n",
        "    else:\n",
        "      span.append((int(row[2]), int(row[3])))\n",
        "      technique.append(\"Slogans\")\n",
        "  spans.append(span)\n",
        "  techniques.append(technique)\n",
        "  indices.append(article_index)\n",
        "  if mode == 'test':\n",
        "    return spans, techniques, indices\n",
        "  return spans, techniques"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLyM6nBECUgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_spans(article, span, technique):\n",
        "  for index, sp in enumerate(span):\n",
        "    print(technique[index], tag2idx[technique[index]], end=' - ')\n",
        "    print (article[sp[0]: sp[1]])\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gKIOgezhjAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_examples(articles, spans, techniques):\n",
        "  assert len(articles) == len(spans) and len(spans) == len(techniques)\n",
        "  sentences = []\n",
        "  labels = []\n",
        "  for index, article in enumerate(articles):\n",
        "    span = spans[index]\n",
        "    technique = techniques[index]\n",
        "    assert len(technique) == len(span)\n",
        "    for i, sp in enumerate(span):\n",
        "      pt = tag2idx[technique[i]]\n",
        "      sentence = article[sp[0]: sp[1]]\n",
        "      sentences.append(sentence)\n",
        "      labels.append(pt)\n",
        "  return sentences, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuXFQT9rKaMk",
        "colab_type": "code",
        "outputId": "0ef1c127-16e7-4279-e903-c0572631ba75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from transformers import BertForTokenClassification\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "def convert_sentence_to_input_feature(sentence, tokenizer, add_cls_sep=True, max_seq_len=150):\n",
        "  tokenized_sentence = tokenizer.encode_plus(sentence,\n",
        "                                             add_special_tokens=add_cls_sep,\n",
        "                                             max_length=max_seq_len,\n",
        "                                             pad_to_max_length=True,\n",
        "                                             return_attention_mask=True)\n",
        "  return tokenized_sentence['input_ids'], tokenized_sentence['attention_mask']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1psU4R8EaoyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def get_data(articles, spans, techniques):\n",
        "  sentences, labels = get_examples(articles, spans, techniques)\n",
        "  attention_masks = []\n",
        "  inputs = []\n",
        "  lengths = []\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    lengths.append(len(sentence) / 100) # divide by 100 for normalization\n",
        "    input_ids, mask = convert_sentence_to_input_feature(sentence, tokenizer)\n",
        "    inputs.append(input_ids)\n",
        "    attention_masks.append(mask)\n",
        "  \n",
        "  inputs = torch.tensor(inputs)\n",
        "  labels = torch.tensor(labels)\n",
        "  masks = torch.tensor(attention_masks)\n",
        "  lengths = torch.tensor(lengths).float()\n",
        "  tensor_data = TensorDataset(inputs, labels, masks, lengths)\n",
        "  dataloader = DataLoader(tensor_data, batch_size=BATCH_SIZE)\n",
        "  return dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgR-htBfZ6RX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  print(metrics.confusion_matrix(labels_flat, pred_flat))\n",
        "  print(metrics.classification_report(labels_flat, pred_flat))\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N30yt03J9onb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30KR97XXHCzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import RobertaConfig, RobertaModel, BertPreTrainedModel\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
        "    \"roberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin\",\n",
        "    \"roberta-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin\",\n",
        "    \"roberta-large-mnli\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-pytorch_model.bin\",\n",
        "    \"distilroberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-pytorch_model.bin\",\n",
        "    \"roberta-base-openai-detector\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-openai-detector-pytorch_model.bin\",\n",
        "    \"roberta-large-openai-detector\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-openai-detector-pytorch_model.bin\",\n",
        "}\n",
        "\n",
        "class RobertaClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "class CustomRobertaForSequenceClassification(BertPreTrainedModel):\n",
        "    config_class = RobertaConfig\n",
        "    pretrained_model_archive_map = ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n",
        "    base_model_prefix = \"roberta\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.classifier = RobertaClassificationHead(config)\n",
        "        self.length_classifier = nn.Linear(config.num_labels+1, config.num_labels)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        lengths=None\n",
        "    ):\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "        lengths = lengths.unsqueeze(1)\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        if INCLUDE_LENGTH_FEATURE:\n",
        "          logits = torch.cat((logits, lengths), axis=1)\n",
        "          logits = self.length_classifier(logits)\n",
        "        outputs = (logits,) + outputs[2:]\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uCzVrHKdCj0s",
        "colab": {}
      },
      "source": [
        "def train(model, epochs=5):\n",
        "  loss_values = []\n",
        "  for epoch_i in range(0, epochs):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      if step % 100 == 0 and not step == 0:\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_labels = batch[1].to(device)\n",
        "      b_input_mask = batch[2].to(device)\n",
        "      b_lengths = batch[3].to(device)\n",
        "      model.zero_grad()        \n",
        "      outputs = model(b_input_ids, \n",
        "                      token_type_ids=None, \n",
        "                      attention_mask=b_input_mask,\n",
        "                      lengths=b_lengths,\n",
        "                      labels=b_labels)\n",
        "      loss = outputs[0]\n",
        "      total_loss += loss.item()\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "      optimizer.step()\n",
        "      scheduler.step() # TODO\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    for batch in eval_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      b_input_ids, b_labels, b_input_mask, b_lengths = batch\n",
        "      with torch.no_grad():        \n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask,\n",
        "                        lengths=b_lengths)\n",
        "      \n",
        "      logits = outputs[0]\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = b_labels.to('cpu').numpy()\n",
        "      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "      eval_accuracy += tmp_eval_accuracy\n",
        "      nb_eval_steps += 1\n",
        "\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOW1nliOdVKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_predictions(model, dataloader):\n",
        "  model.eval()\n",
        "  predictions , true_labels = [], []\n",
        "  nb_eval_steps = 0\n",
        "  for batch in dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_labels, b_input_mask, b_lengths = batch  \n",
        "    with torch.no_grad():\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, lengths=b_lengths)\n",
        "    logits = logits[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    pred_label = np.argmax(logits, axis=1)\n",
        "    predictions.extend(pred_label)\n",
        "    true_labels.extend(label_ids)\n",
        "  return predictions, true_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUne1lNgJgmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "def get_dev_predictions(model):\n",
        "  test_articles, _ = read_articles(\"dev-articles\")\n",
        "  test_spans, test_techniques = read_test_spans()\n",
        "\n",
        "  test_articles = test_articles[1:]\n",
        "  test_dataloader = get_data(test_articles, test_spans, test_techniques)\n",
        "  pred, _ = get_model_predictions(model, test_dataloader)\n",
        "\n",
        "  with open('predictions.txt', 'w') as fp:\n",
        "    label_file = os.path.join(data_dir, \"dev-task-TC-template.out\")\n",
        "    myfile = open(label_file)\n",
        "    prev_index = -1\n",
        "    tsvreader = csv.reader(myfile, delimiter=\"\\t\")\n",
        "    for i, row in enumerate(tsvreader):\n",
        "      fp.write(row[0] + '\\t' + distinct_techniques[pred[i]] + '\\t' + row[2] + '\\t' + row[3] + '\\n')\n",
        "  files.download('predictions.txt')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx-oM2UcJik8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "def get_test_predictions():\n",
        "  temp_test_articles, test_indices = read_articles(\"test-TC/test-articles\")\n",
        "  test_spans, test_techniques, span_indices = read_test_spans(mode=\"test\")\n",
        "  test_articles = []\n",
        "  span_indices = set(span_indices)\n",
        "  for index, article in enumerate(temp_test_articles):\n",
        "    if test_indices[index] in span_indices:\n",
        "      test_articles.append(article)\n",
        "  print(len(test_articles))\n",
        "  print(len(test_spans))\n",
        "  test_dataloader = get_data(test_articles, test_spans, test_techniques)\n",
        "  pred, _ = get_model_predictions(model, test_dataloader)\n",
        "\n",
        "  with open('predictions.txt', 'w') as fp:\n",
        "    label_file = os.path.join(data_dir, \"test-TC/test-task-TC-template.out\")\n",
        "    myfile = open(label_file)\n",
        "    prev_index = -1\n",
        "    tsvreader = csv.reader(myfile, delimiter=\"\\t\")\n",
        "    for i, row in enumerate(tsvreader):\n",
        "      fp.write(row[0] + '\\t' + distinct_techniques[pred[i]] + '\\t' + row[2] + '\\t' + row[3] + '\\n')\n",
        "  files.download('predictions.txt')\n",
        "\n",
        "# get_test_predictions()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuUfhq4EhjKL",
        "colab_type": "code",
        "outputId": "45420381-6b83-4136-9f30-bb455f94af95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "articles, article_ids = read_articles(\"train-articles\")\n",
        "spans, techniques = read_spans()\n",
        "distinct_techniques = list(set([y for x in techniques for y in x])) # idx to tag\n",
        "tag2idx = {t: i for i, t in enumerate(distinct_techniques)}\n",
        "pprint.pprint(tag2idx)\n",
        "\n",
        "NUM_ARTICLES = len(articles)\n",
        "\n",
        "articles = articles[0:NUM_ARTICLES]\n",
        "spans = spans[0:NUM_ARTICLES]\n",
        "techniques = techniques[0:NUM_ARTICLES]\n",
        "BATCH_SIZE=8\n",
        "\n",
        "seed_val = 1328 # 32\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "indices = np.arange(NUM_ARTICLES)\n",
        "train_articles, eval_articles, train_spans, eval_spans, train_techniques, eval_techniques, train_indices, eval_indices = train_test_split(articles, spans, techniques, indices, test_size=0.1)\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', lower_case=True)\n",
        "\n",
        "train_dataloader = get_data(train_articles, train_spans, train_techniques)\n",
        "eval_dataloader = get_data(eval_articles, eval_spans, eval_techniques)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Appeal_to_Authority': 1,\n",
            " 'Appeal_to_fear-prejudice': 5,\n",
            " 'Bandwagon,Reductio_ad_hitlerum': 4,\n",
            " 'Black-and-White_Fallacy': 2,\n",
            " 'Causal_Oversimplification': 13,\n",
            " 'Doubt': 9,\n",
            " 'Exaggeration,Minimisation': 12,\n",
            " 'Flag-Waving': 10,\n",
            " 'Loaded_Language': 0,\n",
            " 'Name_Calling,Labeling': 11,\n",
            " 'Repetition': 8,\n",
            " 'Slogans': 6,\n",
            " 'Thought-terminating_Cliches': 7,\n",
            " 'Whataboutism,Straw_Men,Red_Herring': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrEXyRFf8uad",
        "colab_type": "code",
        "outputId": "05908772-bb6f-4f3d-a1cd-097ed8b143b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "# from transformers import RobertaForSequenceClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "INCLUDE_LENGTH_FEATURE = False # Whether to include length as feature\n",
        "\n",
        "model = CustomRobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    num_labels = len(distinct_techniques),\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.cuda()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),lr = 3e-5,eps = 1e-8) # ler = 5e-5\n",
        "epochs = 4\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "train(model, epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch   100  of    691.    Elapsed: 0:00:17.\n",
            "  Batch   200  of    691.    Elapsed: 0:00:34.\n",
            "  Batch   300  of    691.    Elapsed: 0:00:51.\n",
            "  Batch   400  of    691.    Elapsed: 0:01:08.\n",
            "  Batch   500  of    691.    Elapsed: 0:01:25.\n",
            "  Batch   600  of    691.    Elapsed: 0:01:42.\n",
            "\n",
            "  Average training loss: 1.69\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch   100  of    691.    Elapsed: 0:00:17.\n",
            "  Batch   200  of    691.    Elapsed: 0:00:34.\n",
            "  Batch   300  of    691.    Elapsed: 0:00:51.\n",
            "  Batch   400  of    691.    Elapsed: 0:01:08.\n",
            "  Batch   500  of    691.    Elapsed: 0:01:25.\n",
            "  Batch   600  of    691.    Elapsed: 0:01:42.\n",
            "\n",
            "  Average training loss: 1.16\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch   100  of    691.    Elapsed: 0:00:17.\n",
            "  Batch   200  of    691.    Elapsed: 0:00:34.\n",
            "  Batch   300  of    691.    Elapsed: 0:00:51.\n",
            "  Batch   400  of    691.    Elapsed: 0:01:08.\n",
            "  Batch   500  of    691.    Elapsed: 0:01:25.\n",
            "  Batch   600  of    691.    Elapsed: 0:01:42.\n",
            "\n",
            "  Average training loss: 0.81\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch   100  of    691.    Elapsed: 0:00:17.\n",
            "  Batch   200  of    691.    Elapsed: 0:00:34.\n",
            "  Batch   300  of    691.    Elapsed: 0:00:51.\n",
            "  Batch   400  of    691.    Elapsed: 0:01:08.\n",
            "  Batch   500  of    691.    Elapsed: 0:01:25.\n",
            "  Batch   600  of    691.    Elapsed: 0:01:42.\n",
            "\n",
            "  Average training loss: 0.58\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.75\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohRb8xLrNH-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_dev_predictions(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6bbCylBVzdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('predictions.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}