{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Context System 1 Technique Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WkH2d0SLPeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49fpc1iF6jcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDlMmtRU1MDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import csv\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import pprint\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyXwpKlEWE4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "home_dir = \"gdrive/My Drive/propaganda_detection\"\n",
        "data_dir = os.path.join(home_dir, \"datasets\")\n",
        "model_dir = os.path.join(home_dir, \"model_dir\")\n",
        "if not os.path.isdir(model_dir):\n",
        "  os.mkdir(model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H93QqgFyxL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read training articles\n",
        "def read_articles(dir_name):\n",
        "  articles = []\n",
        "  train_dir = os.path.join(data_dir, dir_name)\n",
        "  for filename in sorted(os.listdir(train_dir)):\n",
        "    myfile = open(os.path.join(train_dir, filename))\n",
        "    article = myfile.read()\n",
        "    articles.append(article)\n",
        "    myfile.close()\n",
        "  article_ids = []\n",
        "  for filename in sorted(os.listdir(train_dir)):\n",
        "    article_ids.append(filename[7:-4])\n",
        "  \n",
        "  return articles, article_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ72YQ3aVfKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_titles(articles):\n",
        "  titles = []\n",
        "  for article in articles:\n",
        "    title = article.split('\\n')[0]\n",
        "    titles.append(title)\n",
        "  return titles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfBpePjiH_-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read training span labels \n",
        "def read_spans(mode=None):\n",
        "  spans = []\n",
        "  techniques = []\n",
        "  if mode == \"test\":\n",
        "    label_dir = os.path.join(data_dir, \"dev-task-TC-template.out\")\n",
        "  else:\n",
        "    label_dir = os.path.join(data_dir, \"train-labels-task2-technique-classification\")\n",
        "  for filename in sorted(os.listdir(label_dir)):\n",
        "    myfile = open(os.path.join(label_dir, filename))\n",
        "    tsvreader = csv.reader(myfile, delimiter=\"\\t\")\n",
        "    span = []\n",
        "    technique = []\n",
        "    for row in tsvreader:\n",
        "      span.append((int(row[2]), int(row[3])))\n",
        "      if mode == \"test\":\n",
        "        technique.append(\"Slogans\") # DUMMY\n",
        "      else:\n",
        "        technique.append(row[1])\n",
        "    myfile.close()\n",
        "    spans.append(span)\n",
        "    techniques.append(technique)\n",
        "  return spans, techniques"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9FjNDEZ4tnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read training span labels \n",
        "def read_test_spans():\n",
        "  spans = []\n",
        "  techniques = []\n",
        "  label_file = os.path.join(data_dir, \"dev-task-TC-template.out\")\n",
        "  myfile = open(label_file)\n",
        "  prev_index = -1\n",
        "  tsvreader = csv.reader(myfile, delimiter=\"\\t\")\n",
        "\n",
        "  span = []\n",
        "  technique = []\n",
        "  for row in tsvreader:\n",
        "    article_index = int(row[0])\n",
        "    if article_index != prev_index:\n",
        "      if prev_index != -1:\n",
        "        spans.append(span)\n",
        "        techniques.append(technique)\n",
        "      span = []\n",
        "      technique = []\n",
        "      span.append((int(row[2]), int(row[3])))\n",
        "      technique.append(\"Slogans\")\n",
        "      prev_index = article_index\n",
        "    else:\n",
        "      span.append((int(row[2]), int(row[3])))\n",
        "      technique.append(\"Slogans\")\n",
        "  spans.append(span)\n",
        "  techniques.append(technique)\n",
        "  return spans, techniques, "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLyM6nBECUgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_spans(article, span, technique):\n",
        "  for index, sp in enumerate(span):\n",
        "    print(technique[index], tag2idx[technique[index]], end=' - ')\n",
        "    print (article[sp[0]: sp[1]])\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmCzsoB8A5bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_context(article, span, mode=None):\n",
        "  def get_num_words(sentence):\n",
        "    return len(sentence.split(' '))\n",
        "  if mode == \"title\":\n",
        "    return article.split('\\n')[0]\n",
        "  if mode == \"sentence\":\n",
        "    WORD_LEN_LIMIT = 120\n",
        "    li = span[0]\n",
        "    ri = span[1]\n",
        "    span_text = article[li: ri]\n",
        "    num_words = get_num_words(span_text)\n",
        "    if num_words >= WORD_LEN_LIMIT:\n",
        "      return span_text\n",
        "    remaining_len = WORD_LEN_LIMIT - num_words\n",
        "    lhs_words = remaining_len // 2\n",
        "    rhs_words = remaining_len - lhs_words\n",
        "    li -= 1\n",
        "    lcount = 0\n",
        "    while li >= 0 and article[li] != '\\n' and lcount < lhs_words:\n",
        "      if article[li] == ' ':\n",
        "        lcount += 1\n",
        "      li -= 1\n",
        "    ri += 1\n",
        "    rcount = 0\n",
        "    while ri < len(article) and article[ri] != '\\n' and rcount < rhs_words:\n",
        "      if article[ri] == ' ':\n",
        "        rcount += 1\n",
        "      ri += 1\n",
        "    return article[li+1: ri - 1] \n",
        "\n",
        "  return \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gKIOgezhjAE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change max_seq_len as well in convert_sentence_to_input_feature function\n",
        "# Set max seq length as 150 when using Title Context\n",
        "def get_examples(articles, spans, techniques, context_mode=\"sentence\"):\n",
        "  assert len(articles) == len(spans) and len(spans) == len(techniques)\n",
        "  sentences = []\n",
        "  labels = []\n",
        "  sent_contexts = []\n",
        "  for index, article in enumerate(articles):\n",
        "    span = spans[index]\n",
        "    technique = techniques[index]\n",
        "    assert len(technique) == len(span)\n",
        "    for i, sp in enumerate(span):\n",
        "      pt = tag2idx[technique[i]]\n",
        "      sentence = article[sp[0]: sp[1]]\n",
        "      sentences.append(sentence)\n",
        "      labels.append(pt)\n",
        "      context = get_context(article, sp, context_mode)\n",
        "      sent_contexts.append(context)\n",
        "  return sentences, labels, sent_contexts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuXFQT9rKaMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForTokenClassification\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "def convert_sentence_to_input_feature(sentence, tokenizer, add_cls_sep=True, max_seq_len=256, title=None):\n",
        "  tokenized_sentence = tokenizer.encode_plus(sentence,\n",
        "                                             text_pair=title,\n",
        "                                             add_special_tokens=add_cls_sep,\n",
        "                                             max_length=max_seq_len,\n",
        "                                             pad_to_max_length=True,\n",
        "                                             return_attention_mask=True)\n",
        "  attention_mask = tokenized_sentence['attention_mask']\n",
        "  input_ids = tokenized_sentence['input_ids']\n",
        "  token_type_ids = [0] * max_seq_len\n",
        "  li = np.sum(attention_mask)\n",
        "  si = 0\n",
        "  for i in range(max_seq_len):\n",
        "    if input_ids[i] == tokenizer.sep_token_id:\n",
        "      si = i+1\n",
        "      break\n",
        "  for i in range(li - si):\n",
        "    token_type_ids[i+si] = 1 \n",
        "\n",
        "  return input_ids, attention_mask, token_type_ids  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1psU4R8EaoyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "def get_data(articles, spans, techniques):\n",
        "  sentences, labels, titles = get_examples(articles, spans, techniques)\n",
        "  attention_masks = []\n",
        "  inputs = []\n",
        "  token_type_ids = []\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    input_ids, mask, segment_ids = convert_sentence_to_input_feature(sentence, tokenizer, title=titles[i])\n",
        "    inputs.append(input_ids)\n",
        "    attention_masks.append(mask)\n",
        "    token_type_ids.append(segment_ids)\n",
        "  inputs = torch.tensor(inputs)\n",
        "  labels = torch.tensor(labels)\n",
        "  masks = torch.tensor(attention_masks)\n",
        "  token_type_ids = torch.tensor(token_type_ids)\n",
        "  tensor_data = TensorDataset(inputs, labels, masks, token_type_ids)\n",
        "  dataloader = DataLoader(tensor_data, batch_size=BATCH_SIZE)\n",
        "  return dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgR-htBfZ6RX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  print(metrics.confusion_matrix(labels_flat, pred_flat))\n",
        "  print(metrics.classification_report(labels_flat, pred_flat))\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N30yt03J9onb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uCzVrHKdCj0s",
        "colab": {}
      },
      "source": [
        "def train(model, epochs=5):\n",
        "  loss_values = []\n",
        "  for epoch_i in range(0, epochs):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      if step % 100 == 0 and not step == 0:\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "      b_input_ids = batch[0].to(device)\n",
        "      b_labels = batch[1].to(device)\n",
        "      b_input_mask = batch[2].to(device)\n",
        "      b_token_type_ids = batch[3].to(device)\n",
        "      model.zero_grad()        \n",
        "      outputs = model(b_input_ids, \n",
        "                      # token_type_ids=b_token_type_ids, \n",
        "                      attention_mask=b_input_mask, \n",
        "                      labels=b_labels)\n",
        "      loss = outputs[0]\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      scheduler.step() # TODO\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    loss_values.append(avg_train_loss)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    for batch in eval_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      b_input_ids, b_labels, b_input_mask, b_token_type_ids = batch\n",
        "      with torch.no_grad():        \n",
        "        outputs = model(b_input_ids, \n",
        "                        # token_type_ids=b_token_type_ids, \n",
        "                        attention_mask=b_input_mask)\n",
        "      \n",
        "      logits = outputs[0]\n",
        "\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = b_labels.to('cpu').numpy()\n",
        "      \n",
        "      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "      \n",
        "      eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "      nb_eval_steps += 1\n",
        "\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOW1nliOdVKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_predictions(model, dataloader):\n",
        "  model.eval()\n",
        "  predictions , true_labels = [], []\n",
        "  nb_eval_steps = 0\n",
        "  for batch in dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_labels, b_input_mask, b_token_type_ids = batch  \n",
        "    with torch.no_grad():\n",
        "      logits = model(b_input_ids, attention_mask=b_input_mask)\n",
        "      # logits = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask)\n",
        "    logits = logits[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    pred_label = np.argmax(logits, axis=1)\n",
        "    predictions.extend(pred_label)\n",
        "    true_labels.extend(label_ids)\n",
        "  return predictions, true_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs-aNAVK5Ina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "def get_dev_predictions(model):\n",
        "  test_articles, _ = read_articles(\"dev-articles\")\n",
        "  test_spans, test_techniques = read_test_spans()\n",
        "\n",
        "  test_articles = test_articles[1:]\n",
        "  test_dataloader = get_data(test_articles, test_spans, test_techniques)\n",
        "  pred, _ = get_model_predictions(model, test_dataloader)\n",
        "\n",
        "  with open('predictions.txt', 'w') as fp:\n",
        "    label_file = os.path.join(data_dir, \"dev-task-TC-template.out\")\n",
        "    myfile = open(label_file)\n",
        "    prev_index = -1\n",
        "    tsvreader = csv.reader(myfile, delimiter=\"\\t\")\n",
        "    for i, row in enumerate(tsvreader):\n",
        "      fp.write(row[0] + '\\t' + distinct_techniques[pred[i]] + '\\t' + row[2] + '\\t' + row[3] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuUfhq4EhjKL",
        "colab_type": "code",
        "outputId": "bc01d348-5ea4-425d-ef78-cd8f9d0a1075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "articles, article_ids = read_articles(\"train-articles\")\n",
        "spans, techniques = read_spans()\n",
        "distinct_techniques = list(set([y for x in techniques for y in x])) # idx to tag\n",
        "tag2idx = {t: i for i, t in enumerate(distinct_techniques)}\n",
        "pprint.pprint(tag2idx)\n",
        "NUM_ARTICLES = len(articles)\n",
        "\n",
        "articles = articles[0:NUM_ARTICLES]\n",
        "spans = spans[0:NUM_ARTICLES]\n",
        "techniques = techniques[0:NUM_ARTICLES]\n",
        "BATCH_SIZE=8\n",
        "\n",
        "# seed_val = 32\n",
        "seed_val = 1328\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "indices = np.arange(NUM_ARTICLES)\n",
        "\n",
        "train_articles, eval_articles, train_spans, eval_spans, train_techniques, eval_techniques, train_indices, eval_indices = train_test_split(articles, spans, techniques, indices, test_size=0.1)\n",
        "\n",
        "#### Change context mode in get_examples function ####\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', lower_case=True)\n",
        "\n",
        "\n",
        "train_dataloader = get_data(train_articles, train_spans, train_techniques)\n",
        "eval_dataloader = get_data(eval_articles, eval_spans, eval_techniques)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Appeal_to_Authority': 10,\n",
            " 'Appeal_to_fear-prejudice': 13,\n",
            " 'Bandwagon,Reductio_ad_hitlerum': 5,\n",
            " 'Black-and-White_Fallacy': 2,\n",
            " 'Causal_Oversimplification': 8,\n",
            " 'Doubt': 7,\n",
            " 'Exaggeration,Minimisation': 4,\n",
            " 'Flag-Waving': 3,\n",
            " 'Loaded_Language': 6,\n",
            " 'Name_Calling,Labeling': 0,\n",
            " 'Repetition': 11,\n",
            " 'Slogans': 1,\n",
            " 'Thought-terminating_Cliches': 9,\n",
            " 'Whataboutism,Straw_Men,Red_Herring': 12}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrEXyRFf8uad",
        "colab_type": "code",
        "outputId": "2cd079e2-57a6-4f95-9dd3-412d06df9502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    num_labels = len(distinct_techniques),\n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.cuda()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),lr = 3e-5,eps = 1e-8) # ler = 5e-5\n",
        "epochs = 3\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "train(model, epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "  Batch   100  of    691.    Elapsed: 0:01:12.\n",
            "  Batch   200  of    691.    Elapsed: 0:02:25.\n",
            "  Batch   300  of    691.    Elapsed: 0:03:37.\n",
            "  Batch   400  of    691.    Elapsed: 0:04:50.\n",
            "  Batch   500  of    691.    Elapsed: 0:06:02.\n",
            "  Batch   600  of    691.    Elapsed: 0:07:15.\n",
            "\n",
            "  Average training loss: 1.95\n",
            "  Training epcoh took: 0:08:20\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.46\n",
            "  Validation took: 0:00:17\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "  Batch   100  of    691.    Elapsed: 0:01:13.\n",
            "  Batch   200  of    691.    Elapsed: 0:02:25.\n",
            "  Batch   400  of    691.    Elapsed: 0:04:50.\n",
            "  Batch   500  of    691.    Elapsed: 0:06:03.\n",
            "  Batch   600  of    691.    Elapsed: 0:07:16.\n",
            "\n",
            "  Average training loss: 1.63\n",
            "  Training epcoh took: 0:08:21\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.64\n",
            "  Validation took: 0:00:17\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "  Batch   100  of    691.    Elapsed: 0:01:13.\n",
            "  Batch   200  of    691.    Elapsed: 0:02:25.\n",
            "  Batch   300  of    691.    Elapsed: 0:03:38.\n",
            "  Batch   500  of    691.    Elapsed: 0:06:03.\n",
            "  Batch   600  of    691.    Elapsed: 0:07:16.\n",
            "\n",
            "  Average training loss: 1.22\n",
            "  Training epcoh took: 0:08:21\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.67\n",
            "  Validation took: 0:00:17\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34dOS-z04d7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_dev_predictions(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJh1sSFU5mvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('predictions.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}